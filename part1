import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from arch import arch_model
from hmmlearn.hmm import GaussianHMM
from scipy.stats import genpareto
import datetime

# Configure the app page
st.set_page_config(page_title="FX Thresholding App", layout="wide")

# Utility functions for threshold calculations
def rolling_quantile(vol_series, window, q):
    """Compute rolling quantile (e.g. 95th percentile) over a moving window."""
    return vol_series.rolling(window).quantile(q)

def garch_evt(returns, tail_pct=0.99):
    """
    Fit a GARCH(1,1) model to returns and use EVT (Peak-Over-Threshold) on residuals 
    to estimate an extreme volatility threshold corresponding to the given tail probability.
    """
    # Fit GARCH model on returns (%)
    am = arch_model(returns * 100, vol='Garch', p=1, q=1) 
    res = am.fit(disp='off')
    # Get standardized residuals
    std_resid = res.resid / res.conditional_volatility
    std_resid = std_resid.dropna()
    # Determine threshold for tail (90th percentile of residuals)
    u = np.quantile(std_resid, 0.90)
    excess = std_resid[std_resid > u] - u
    # Fit Generalized Pareto Distribution to excesses
    c, loc, scale = genpareto.fit(excess, floc=0)
    # Compute the value-at-risk for the desired tail probability
    tail_prob = tail_pct  # e.g. 0.990 for 99th percentile
    # Proportion of data beyond u
    prob_exc = np.mean(std_resid > u)
    # Calculate the quantile of the fitted GPD corresponding to tail_pct
    p_exc = (tail_prob - (1 - prob_exc)) / prob_exc
    var_est = genpareto.ppf(p_exc, c, loc=0, scale=scale)
    # Return threshold in original volatility units (undo scaling by 100)
    return (u + var_est) / 100.0

def detect_regimes(vol_series, n_states=3):
    """
    Fit a Hidden Markov Model on the volatility series to detect latent volatility regimes.
    Returns the state labels for each time point, the index of the high-volatility state, 
    and the mean volatility of that high-vol state.
    """
    model = GaussianHMM(n_components=n_states, covariance_type='full', n_iter=200, random_state=42)
    data = vol_series.values.reshape(-1, 1)
    model.fit(data)
    states = model.predict(data)
    # Determine which state corresponds to highest volatility (max mean)
    state_means = {state: data[states == state].mean() for state in np.unique(states)}
    high_state = max(state_means, key=state_means.get)
    return states, high_state, state_means[high_state]

# Sidebar: Data Upload and Settings
st.sidebar.header("Upload & Settings")
uploaded_file = st.sidebar.file_uploader("Upload FX Data CSV", 
                                         type="csv", 
                                         help="CSV with columns: Date, Open, High, Low, Close, OHLCVolatility, Currency")
if not uploaded_file:
    st.sidebar.info("Please upload a CSV file to proceed.")
    st.stop()

# Load and preprocess data
df = pd.read_csv(uploaded_file, parse_dates=['Date']).sort_values('Date')
# Calculate daily volatility from OHLC volatility (assuming OHLCVolatility is annualized)
df['DailyVol'] = df['OHLCVolatility'] / np.sqrt(252)

# Sidebar: select currency pair to analyze
currencies = sorted(df["Currency"].unique())
selected_ccy = st.sidebar.selectbox("Select Currency Pair", currencies)
# Filter data for the selected currency
dfc = df[df["Currency"] == selected_ccy].copy().reset_index(drop=True)
# Compute log returns for that currency (for use in GARCH/EVT)
dfc["LogReturn"] = np.log(dfc["Close"] / dfc["Close"].shift(1))
dfc.dropna(inplace=True)

# Apply HMM to detect volatility regimes on this currency's data
states, high_regime, high_regime_mean = detect_regimes(dfc['DailyVol'], n_states=3)
dfc['Regime'] = states  # add regime labels to data
# (Optional) Map regime labels to names for interpretability
regime_names = {high_regime: "High Volatility"}
# Name the other regimes as "Low" and "Medium" based on their means
other_states = [s for s in np.unique(states) if s != high_regime]
# Sort the other states by mean volatility
other_states.sort(key=lambda s: dfc[dfc['Regime']==s]['DailyVol'].mean())
if len(other_states) >= 1: regime_names[other_states[0]] = "Low Volatility"
if len(other_states) >= 2: regime_names[other_states[1]] = "Medium Volatility"
# Create a new column with named regimes for plotting
dfc['RegimeName'] = dfc['Regime'].map(regime_names)

# Calculate dynamic thresholds for comparison
window = 90  # rolling window in days for quantile
quantile_level = 0.95  # 95th percentile for alert threshold
dfc['DynamicThreshold'] = rolling_quantile(dfc['DailyVol'], window, quantile_level)
# EVT-based threshold (based on entire history of returns for simplicity here)
evt_threshold = garch_evt(dfc['LogReturn'], tail_pct=0.990)

# Derive a static manual threshold for this currency:
# For example, use the long-term average volatility times a factor, or from predefined group table
manual_threshold = dfc['DailyVol'].mean()  # here we use mean as a simple static threshold
