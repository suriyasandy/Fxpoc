# app.py

import streamlit as st
import pandas as pd
import numpy as np
import itertools, datetime
from pandas.tseries.offsets import BDay
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import plotly.express as px
import plotly.graph_objects as go
from arch import arch_model
from scipy.stats import genpareto
from hmmlearn.hmm import GaussianHMM

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UTILITIES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SQRT252 = np.sqrt(252)
MANUAL_BANDS = {
    'Low':      (0.00, 0.07),
    'Medium':   (0.07, 0.50),
    'High':     (0.50, 0.60),
    'VeryHigh': (0.60, None)
}

def compute_manual_groups(df):
    df2 = df.copy()
    df2['DailyVol'] = df2['OHLCVolatility'] / SQRT252
    mv = (
        df2.groupby('Currency')['DailyVol']
           .mean()
           .reset_index(name='MeanDailyVol')
    )
    def band(v):
        for b,(lo,hi) in MANUAL_BANDS.items():
            if hi is None and v>=lo: return b
            if lo<=v<hi:            return b
        return None
    mv['Band'] = mv['MeanDailyVol'].map(band)
    bt = (
        mv.groupby('Band')['MeanDailyVol']
          .max()
          .reset_index(name='BandThreshold')
    )
    return mv, bt

def rolling_quantile(s, w, q): 
    return s.rolling(w).quantile(q)

def evt_vol_threshold(vol, u_pct=0.90, tail_pct=0.995):
    u = vol.quantile(u_pct)
    exc = vol[vol>u] - u
    if exc.empty:
        return u
    c, loc, scale = genpareto.fit(exc, floc=0)
    # map tail_pct to GPD ppf
    var = genpareto.ppf(tail_pct, c, loc=0, scale=scale)
    return u + var

def detect_regimes(vol, n_states):
    clean = vol.replace([np.inf,-np.inf],np.nan).dropna()
    if clean.empty:
        return np.zeros(len(vol),int), 0, np.nan
    arr = clean.values.reshape(-1,1)
    m = GaussianHMM(n_components=n_states, covariance_type='full', n_iter=200)
    m.fit(arr)
    states = m.predict(arr)
    means = {s: arr[states==s].mean() for s in np.unique(states)}
    high = max(means, key=means.get)
    # reindex to full
    s = pd.Series(states, index=clean.index)
    full = s.reindex(vol.index).ffill().bfill().astype(int)
    return full.values, high, means[high]

def smooth_regimes(raw, min_run=5):
    s = pd.Series(raw)
    runs = (s != s.shift()).cumsum()
    sizes = s.groupby(runs).transform('size')
    s[sizes < min_run] = np.nan
    return s.ffill().bfill().astype(int).values

def calibrate_regime(vol, lr, target, windows, qs, tails):
    # rolling
    best_wq = min(
        ((abs((vol > rolling_quantile(vol, w, q)).mean() - target), (w,q))
         for w,q in itertools.product(windows, qs)),
        key=lambda x: x[0]
    )[1]
    # EVT
    best_t = min(
        ((abs((vol > evt_vol_threshold(vol, 0.90, t)).mean() - target), t)
         for t in tails),
        key=lambda x: x[0]
    )[1]
    return {'window': best_wq[0], 'quantile': best_wq[1], 'tail': best_t}

def build_features(vol):
    df = pd.DataFrame({'Vol': vol})
    df['LogRet'] = np.log(df['Vol']/df['Vol'].shift(1))
    for w in [7,14,30]:
        df[f'Vol_MA_{w}']  = df['Vol'].rolling(w).mean()
        df[f'Vol_STD_{w}'] = df['Vol'].rolling(w).std()
    return df.dropna()

def fit_autoencoder(X, encoding_dim=5, epochs=20):
    scaler = StandardScaler(); Xs = scaler.fit_transform(X)
    model = Sequential([
        Dense(encoding_dim*2, activation='relu', input_shape=(X.shape[1],)),
        Dense(encoding_dim, activation='relu'),
        Dense(encoding_dim*2, activation='relu'),
        Dense(X.shape[1], activation='linear'),
    ])
    model.compile(optimizer=Adam(0.001), loss='mse')
    model.fit(Xs, Xs, epochs=epochs, batch_size=32, verbose=0)
    rec = model.predict(Xs)
    mse = np.mean((Xs-rec)**2, axis=1)
    return mse

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  STREAMLIT APP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(page_title="FX Vol PoC", layout="wide")
st.title("ðŸŽ¯ FX Volatility Thresholding & Shock Simulation")

# â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("1) Upload FX Data")
f = st.sidebar.file_uploader("CSV (Date,Open,High,Low,Close,OHLCVolatility,Currency)", type="csv")
if not f:
    st.sidebar.info("Awaiting FX dataâ€¦")
    st.stop()

# Load & prep
df = pd.read_csv(f, parse_dates=['Date']).sort_values(['Currency','Date'])
df['DailyVol'] = df['OHLCVolatility'] / SQRT252

# Manual groups
mv_df, bt_df = compute_manual_groups(df)

# Build per-currency series
b_ser = df[df.Currency==df.Currency.unique()[0]].set_index('Date')['DailyVol']  # placeholder
pivot = df.pivot(index='Date', columns='Currency', values='DailyVol')
codes = sorted(pivot.columns)
for c in codes: 
    pass

# FX crosses
crosses = [f"{a}/{b}" for i,a in enumerate(codes) for b in codes[i+1:]]
sel_cross = st.sidebar.selectbox("2) Select FX Cross", crosses)
base, quote = sel_cross.split('/')

b_ser = df[df.Currency==base].set_index('Date')['DailyVol']
q_ser = df[df.Currency==quote].set_index('Date')['DailyVol']
idx   = b_ser.index.intersection(q_ser.index)
cross_vol = np.sqrt(b_ser.loc[idx]**2 + q_ser.loc[idx]**2)
cross_lr  = np.log(
    df[df.Currency==quote].set_index('Date')['Close'].loc[idx] /
    df[df.Currency==base].set_index('Date')['Close'].loc[idx]
)

# Regime detection
n_states = st.sidebar.slider("3) HMM States", 2, 4, 2)
rb,_,_ = detect_regimes(b_ser, n_states)
rq,_,_ = detect_regimes(q_ser, n_states)
rx,hd,_ = detect_regimes(cross_vol, n_states)
sb = smooth_regimes(rb); sq = smooth_regimes(rq); sx = smooth_regimes(rx)

# Build cross DataFrame
dfc = pd.DataFrame({
    'Vol': cross_vol,
    'LogRet': cross_lr,
    'Regime': sx
}, index=idx)
dfc['Label'] = dfc.Regime.map(lambda x: f"Regime {x}")

# Manual threshold for this cross
b1 = mv_df.set_index('Currency')['Band'][base]
b2 = mv_df.set_index('Currency')['Band'][quote]
order = ['Low','Medium','High','VeryHigh']
man_group = b1 if order.index(b1) > order.index(b2) else b2
man_thr = bt_df.set_index('Band')['BandThreshold'][man_group]

# Calibration & dynamic thresholds
target_rate  = st.sidebar.slider("4) Target Alert Rate", 0.01, 0.20, 0.05, 0.01)
roll_windows = [30,60,90,120]; roll_qs = [0.90,0.95,0.99]; evt_tails = [0.990,0.995,0.999]

calib = {}
for r, grp in dfc.groupby('Regime'):
    calib[r] = calibrate_regime(
        grp['Vol'], grp['LogRet'].dropna().values,
        target_rate, roll_windows, roll_qs, evt_tails
    )

# Apply thresholds
dfc['Thr_Warning'] = np.nan
dfc['Thr_Alert']   = np.nan
for r, grp in dfc.groupby('Regime'):
    w,q = calib[r]['window'], calib[r]['quantile']
    dfc.loc[grp.index,'Thr_Warning'] = rolling_quantile(grp['Vol'], w, 0.90)
    dfc.loc[grp.index,'Thr_Alert']   = rolling_quantile(grp['Vol'], w, q)
dfc['Thr_Critical'] = dfc.Regime.map(lambda r: evt_vol_threshold(
    dfc[dfc.Regime==r]['Vol'], 0.90, calib[r]['tail']))

# ML consensus for reference
feat = build_features(dfc['Vol'])
X = feat.drop(columns=['LogRet']).values
# IsolationForest
ifr = IsolationForest(contamination=target_rate, random_state=0).fit(X)
feat['IF_Anom'] = (ifr.predict(X) == -1).astype(int)
# SVM
svm = OneClassSVM(nu=target_rate, kernel='rbf', gamma='auto').fit(X)
feat['SVM_Anom'] = (svm.predict(X) == -1).astype(int)
# AE
mse = fit_autoencoder(X, encoding_dim=3, epochs=30)
thr95 = np.quantile(mse,0.95)
feat['AE_Anom'] = (mse > thr95).astype(int)
feat['ML_Consensus'] = (feat[['IF_Anom','SVM_Anom','AE_Anom']].sum(axis=1) >= 2).astype(int)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  TABS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab1, tab2, tab3, tab4 = st.tabs([
    "ðŸ“‹ Overview",
    "ðŸ“Š Dashboard",
    "ðŸ§  ML & Consensus",
    "ðŸš€ Shock Simulation"
])

# â”€ Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    st.header("Why Dynamic & Shock-Aware Thresholds?")
    st.markdown("""
    1. **Manual Buckets** (static) canâ€™t adapt when volatility regimes shift.
    2. **Dynamic** = rolling quantile + EVT tail-risk *per regime*.
    3. **Shock Simulation** shows how...  
       > when a real shock hits, our dynamic thresholds *re-calibrate* daily.
    """)

# â”€ Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab2:
    st.header(f"Threshold Dashboard: {sel_cross}")
    c0,c1,c2,c3,c4,c5 = st.columns(6)
    c0.metric("Manual Thr",      f"{man_thr:.4f}")
    c1.metric("Latest Vol",      f"{dfc['Vol'][-1]:.4f}")
    c2.metric("Warning Thr",     f"{dfc['Thr_Warning'][-1]:.4f}")
    c3.metric("Alert Thr",       f"{dfc['Thr_Alert'][-1]:.4f}")
    c4.metric("Critical Thr",    f"{dfc['Thr_Critical'][-1]:.4f}")
    c5.metric("Dyn Consensus",   f"{feat['ML_Consensus'].mean():.1%}")

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=dfc.index, y=dfc['Vol'],
                             name='Vol', line=dict(color='blue')))
    fig.add_trace(go.Scatter(x=dfc.index, y=dfc['Thr_Alert'],
                             name='Alert (Dynamic)', line=dict(color='orange', dash='dash')))
    fig.add_trace(go.Scatter(x=dfc.index, y=dfc['Thr_Critical'],
                             name='Critical (EVT)',     line=dict(color='red', dash='dot')))
    fig.add_hline(y=man_thr, line_dash='longdash', line_color='black',
                  annotation_text="Manual Thr", annotation_position="bottom right")
    fig.update_layout(
        xaxis=dict(rangeslider=dict(visible=True)),
        yaxis_title="Daily Volatility",
        height=500
    )
    st.plotly_chart(fig, use_container_width=True)

# â”€ ML & Consensus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab3:
    st.header("ðŸ§  ML & Consensus Breach Rates")
    st.subheader("Overall Breach Comparison")
    br = pd.Series({
        'Manual Thr':    (dfc['Vol']>man_thr).mean(),
        'Dyn Alert':     (dfc['Vol']>dfc['Thr_Alert']).mean(),
        'Dyn EVT':       (dfc['Vol']>dfc['Thr_Critical']).mean(),
        'ML Consensus':  feat['ML_Consensus'].mean()
    })
    st.bar_chart(br)

    st.subheader("Trend of ML Consensus")
    fig2 = go.Figure()
    fig2.add_trace(go.Scatter(
        x=feat.index, y=feat['ML_Consensus'],
        mode='lines', name='ML Consensus', line=dict(color='purple')
    ))
    # reference lines
    for nm,val,col in [
        ("Manual",    br['Manual Thr'],    'black'),
        ("Dyn Alert", br['Dyn Alert'],     'orange'),
        ("Dyn EVT",   br['Dyn EVT'],       'red')
    ]:
        fig2.add_hline(y=val, line_dash='dot', line_color=col,
                       annotation_text=nm, annotation_position="top left")
    fig2.update_layout(
        yaxis=dict(title='Breach Rate / Consensus Flag'),
        xaxis_title="Date",
        height=400
    )
    st.plotly_chart(fig2, use_container_width=True)

# â”€ Shock Simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab4:
    st.header("ðŸš€ Shock Simulation & Re-calibration")

    # snapshot & shock dates
    min_date = dfc.index[0].date()
    max_date = dfc.index[-1].date()
    snap_date  = st.date_input(
        "Snapshot Date (pre-shock)", 
        value=(max_date - datetime.timedelta(days=30)),
        min_value=min_date, max_value=max_date - datetime.timedelta(days=1)
    )
    shock_date = st.date_input(
        "Shock End Date", 
        value=max_date,
        min_value=snap_date + datetime.timedelta(days=1),
        max_value=max_date
    )

    # convert to actual timestamps in dfc
    snap_ts  = max([d for d in dfc.index if d.date() <= snap_date])
    shock_ts = max([d for d in dfc.index if d.date() <= shock_date])

    # shock parameters
    shock_leg    = st.multiselect("Which leg(s) to shock?", 
                                  ['Base Leg','Quote Leg'], default=['Quote Leg'])
    shock_factor = st.slider("Shock Magnitude (Ã— real vol)", 1.0, 5.0, 2.0)

    # build shocked vol series
    b2 = b_ser.copy(); q2 = q_ser.copy()
    mask = (b2.index > snap_ts) & (b2.index <= shock_ts)
    if 'Base Leg'  in shock_leg: b2.loc[mask] *= shock_factor
    if 'Quote Leg' in shock_leg: q2.loc[mask] *= shock_factor
    vol2 = np.sqrt(b2.loc[idx]**2 + q2.loc[idx]**2)

    # recalc dynamic thresholds on shocked vol
    dfd = dfc.copy()
    dfd['Vol'] = vol2
    dfd['Thr_Alert_shock'] = np.nan
    dfd['Thr_Critical_shock'] = np.nan
    for r,grp in dfd.groupby('Regime'):
        w,q = calib[r]['window'], calib[r]['quantile']
        dfd.loc[grp.index,'Thr_Alert_shock'] = rolling_quantile(grp['Vol'], w, q)
        dfd.loc[grp.index,'Thr_Critical_shock'] = evt_vol_threshold(
            grp['Vol'], 0.90, calib[r]['tail']
        )

    # metrics
    alert_snap = dfc.loc[snap_ts,'Thr_Alert']
    evt_snap   = dfc.loc[snap_ts,'Thr_Critical']
    alert_sh   = dfd.loc[shock_ts,'Thr_Alert_shock']
    evt_sh     = dfd.loc[shock_ts,'Thr_Critical_shock']

    c0,c1,c2,c3,c4 = st.columns(5)
    c0.metric("Manual Thr (static)", f"{man_thr:.4f}")
    c1.metric("Alert Thr @ snap",     f"{alert_snap:.4f}")
    c2.metric("EVT Thr @ snap",       f"{evt_snap:.4f}")
    c3.metric("Alert Thr @ shock",    f"{alert_sh:.4f}")
    c4.metric("EVT Thr @ shock",      f"{evt_sh:.4f}")

    # visualization
    fig3 = go.Figure()
    fig3.add_trace(go.Scatter(
        x=dfc.index, y=dfc['Vol'],
        name="Orig Vol", line=dict(color='lightgrey')
    ))
    fig3.add_trace(go.Scatter(
        x=dfd.index, y=dfd['Vol'],
        name="Shocked Vol", line=dict(color='blue')
    ))
    # shade shock window
    fig3.add_vrect(x0=snap_ts, x1=shock_ts,
                   fillcolor="red", opacity=0.1, line_width=0)
    # thresholds
    fig3.add_hline(y=man_thr, line_dash='longdash', line_color='black',
                   annotation_text="Manual", annotation_position="bottom right")
    fig3.add_hline(y=alert_snap, line_dash='dash', line_color='orange',
                   annotation_text="Alert@Snap", annotation_position="bottom right")
    fig3.add_hline(y=alert_sh,   line_dash='dash', line_color='orange',
                   annotation_text="Alert@PostShock", annotation_position="top right")
    fig3.add_hline(y=evt_snap,   line_dash='dot',  line_color='red',
                   annotation_text="EVT@Snap", annotation_position="bottom right")
    fig3.add_hline(y=evt_sh,     line_dash='dot',  line_color='red',
                   annotation_text="EVT@PostShock", annotation_position="top right")
    fig3.update_layout(
        xaxis_title="Date", yaxis_title="Daily Volatility",
        height=500
    )
    st.plotly_chart(fig3, use_container_width=True)

    st.markdown(f"""
    **Shock Window:** {snap_date:%Y-%m-%d} â†’ {shock_date:%Y-%m-%d}  
    - Manual threshold is **static**.  
    - Dynamic thresholds at snapshot vs. post-shock show **real-time adaptation** to increased volatility.  
    - Shaded area highlights the synthetic shock period, proving your model *re-calibrates* where manual does not.
    """)
