# app.py

import streamlit as st
import pandas as pd
import numpy as np
import itertools, datetime
from pandas.tseries.offsets import BDay
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import plotly.express as px
import plotly.graph_objects as go
from arch import arch_model
from scipy.stats import genpareto
from hmmlearn.hmm import GaussianHMM

# ────────────────────────────────────────────────────────────────────────────────
#  UTILITY FUNCTIONS
# ────────────────────────────────────────────────────────────────────────────────

SQRT252 = np.sqrt(252)
MANUAL_BANDS = {
    'Low':      (0.00, 0.07),
    'Medium':   (0.07, 0.50),
    'High':     (0.50, 0.60),
    'VeryHigh': (0.60, None)
}

def compute_manual_groups(df):
    df2 = df.copy()
    df2['DailyVol'] = df2['OHLCVolatility']/SQRT252
    mv = (
        df2.groupby('Currency')['DailyVol']
           .mean()
           .reset_index(name='MeanDailyVol')
    )
    def band(v):
        for b,(lo,hi) in MANUAL_BANDS.items():
            if hi is None and v>=lo: return b
            if lo<=v<hi:            return b
        return None
    mv['Band'] = mv['MeanDailyVol'].map(band)
    bt = (
        mv.groupby('Band')['MeanDailyVol']
          .max()
          .reset_index(name='BandThreshold')
    )
    return mv, bt

def rolling_quantile(s, w, q):
    return s.rolling(w).quantile(q)

def evt_vol_threshold(vol, u_pct=0.90, tail_pct=0.995):
    u = vol.quantile(u_pct)
    exc = vol[vol>u] - u
    if exc.empty:
        return u
    c, loc, scale = genpareto.fit(exc, floc=0)
    var = genpareto.ppf(tail_pct, c, loc=0, scale=scale)
    return u + var

def detect_regimes(vol, n_states):
    clean = vol.replace([np.inf,-np.inf],np.nan).dropna()
    if clean.empty:
        return np.zeros(len(vol),int), 0, np.nan
    arr = clean.values.reshape(-1,1)
    m = GaussianHMM(n_components=n_states, covariance_type='full', n_iter=200)
    m.fit(arr)
    states = m.predict(arr)
    means = {s: arr[states==s].mean() for s in np.unique(states)}
    high = max(means, key=means.get)
    s = pd.Series(states, index=clean.index)
    full = s.reindex(vol.index).ffill().bfill().astype(int)
    return full.values, high, means[high]

def smooth_regimes(raw, min_run=5):
    s = pd.Series(raw)
    runs = (s != s.shift()).cumsum()
    lengths = s.groupby(runs).transform('size')
    s[lengths<min_run] = np.nan
    return s.ffill().bfill().astype(int).values

def calibrate_regime(vol, lr, target, windows, qs, tails):
    best_wq = min(
        ((abs((vol>rolling_quantile(vol,w,q)).mean() - target),(w,q))
         for w,q in itertools.product(windows,qs)),
        key=lambda x: x[0]
    )[1]
    best_t = min(
        ((abs((vol>evt_vol_threshold(vol,0.90,t)).mean() - target),t)
         for t in tails),
        key=lambda x: x[0]
    )[1]
    return {'window':best_wq[0],'quantile':best_wq[1],'tail':best_t}

def build_features(vol):
    df = pd.DataFrame({'Vol':vol}).dropna()
    df['LogRet'] = np.log(df['Vol']/df['Vol'].shift(1))
    for w in [7,14,30]:
        df[f'Vol_MA_{w}']  = df['Vol'].rolling(w).mean()
        df[f'Vol_STD_{w}'] = df['Vol'].rolling(w).std()
    return df.dropna()

def fit_autoencoder(X, encoding_dim=5, epochs=20):
    scaler = StandardScaler(); Xs = scaler.fit_transform(X)
    model = Sequential([
        Dense(encoding_dim*2, activation='relu', input_shape=(X.shape[1],)),
        Dense(encoding_dim, activation='relu'),
        Dense(encoding_dim*2, activation='relu'),
        Dense(X.shape[1], activation='linear'),
    ])
    model.compile(optimizer=Adam(0.001), loss='mse')
    model.fit(Xs,Xs,epochs=epochs,batch_size=32,verbose=0)
    rec = model.predict(Xs)
    return np.mean((Xs-rec)**2,axis=1)

# ────────────────────────────────────────────────────────────────────────────────
#  STREAMLIT APP
# ────────────────────────────────────────────────────────────────────────────────

st.set_page_config(page_title="FX Vol PoC", layout="wide")
st.title("🎯 FX Volatility Thresholding & Shock Simulation")

# ─ Sidebar: Upload & Settings ───────────────────────────────────────────────────
st.sidebar.header("1) Upload FX Data")
file = st.sidebar.file_uploader("CSV (Date,Open,High,Low,Close,OHLCVolatility,Currency)", type="csv")
if not file:
    st.sidebar.info("Upload your FX data first.")
    st.stop()

df = pd.read_csv(file, parse_dates=['Date']).sort_values(['Currency','Date'])
df['DailyVol'] = df['OHLCVolatility']/SQRT252

# Manual grouping
mv_df, bt_df = compute_manual_groups(df)

# Available currencies & crosses
pivot = df.pivot(index='Date', columns='Currency', values='DailyVol')
codes = sorted(pivot.columns)
crosses = [f"{a}/{b}" for i,a in enumerate(codes) for b in codes[i+1:]]
sel_cross = st.sidebar.selectbox("2) Select FX Cross", crosses)
base,quote = sel_cross.split('/')

# Extract base & quote series
b_ser = df[df.Currency==base].set_index('Date')['DailyVol']
q_ser = df[df.Currency==quote].set_index('Date')['DailyVol']
idx   = b_ser.index.intersection(q_ser.index)
cross_vol = np.sqrt(b_ser.loc[idx]**2 + q_ser.loc[idx]**2)
cross_lr  = np.log(
    df[df.Currency==quote].set_index('Date')['Close'].loc[idx] /
    df[df.Currency==base ].set_index('Date')['Close'].loc[idx]
)

# Regimes
n_states = st.sidebar.slider("3) HMM States",2,4,2)
rb,_,_ = detect_regimes(b_ser, n_states)
rq,_,_ = detect_regimes(q_ser, n_states)
rx,_,_ = detect_regimes(cross_vol, n_states)
sb = smooth_regimes(rb); sq = smooth_regimes(rq); sx = smooth_regimes(rx)

# Build cross DataFrame
dfc = pd.DataFrame({
    'Vol': cross_vol,
    'LogRet': cross_lr,
    'Regime': sx
}, index=idx)
dfc['Label'] = dfc.Regime.map(lambda x:f"Regime {x}")

# Manual threshold for this cross
b1 = mv_df.set_index('Currency')['Band'][base]
b2 = mv_df.set_index('Currency')['Band'][quote]
order=['Low','Medium','High','VeryHigh']
man_group = b1 if order.index(b1)>order.index(b2) else b2
man_thr    = bt_df.set_index('Band')['BandThreshold'][man_group]

# Calibration targets
target_rate  = st.sidebar.slider("4) Target Alert Rate",0.01,0.20,0.05,0.01)
roll_windows = [30,60,90,120]; roll_qs=[0.90,0.95,0.99]; evt_tails=[0.990,0.995,0.999]

# Per-regime calibration
calib = {}
for r,grp in dfc.groupby('Regime'):
    calib[r] = calibrate_regime(grp['Vol'],grp['LogRet'].dropna().values,
                                target_rate,roll_windows,roll_qs,evt_tails)

# Apply dynamic thresholds
dfc['Thr_Warning']=np.nan; dfc['Thr_Alert']=np.nan
for r,grp in dfc.groupby('Regime'):
    w,q = calib[r]['window'],calib[r]['quantile']
    dfc.loc[grp.index,'Thr_Warning'] = rolling_quantile(grp['Vol'],w,0.90)
    dfc.loc[grp.index,'Thr_Alert']   = rolling_quantile(grp['Vol'],w,q)
dfc['Thr_Critical'] = dfc.Regime.map(lambda r:
    evt_vol_threshold(dfc[dfc.Regime==r]['Vol'],0.90,calib[r]['tail'])
)

# ML Consensus
feat = build_features(dfc['Vol'])
X = feat.drop(columns=['LogRet']).values
ifr = IsolationForest(contamination=target_rate,random_state=0).fit(X)
feat['IF_Anom']  = (ifr.predict(X)==-1).astype(int)
svm = OneClassSVM(nu=target_rate,kernel='rbf',gamma='auto').fit(X)
feat['SVM_Anom'] = (svm.predict(X)==-1).astype(int)
mse = fit_autoencoder(X,encoding_dim=3,epochs=30)
thr95 = np.quantile(mse,0.95)
feat['AE_Anom']  = (mse>thr95).astype(int)
feat['ML_Consensus'] = (feat[['IF_Anom','SVM_Anom','AE_Anom']].sum(axis=1)>=2).astype(int)

# ────────────────────────────────────────────────────────────────────────────────
#  TABS
# ────────────────────────────────────────────────────────────────────────────────
tab1,tab2,tab3,tab4 = st.tabs([
    "📋 Overview",
    "📊 Dashboard",
    "🧠 ML & Consensus",
    "🚀 Shock Simulation"
])

with tab1:
    st.header("Why Dynamic & Shock-Aware Thresholds?")
    st.markdown("""
    - **Manual buckets** are static → cannot adapt when regimes shift.  
    - **Dynamic** = per-regime rolling‐quantile + EVT tail-risk.  
    - **Shock Simulation** proves that, when volatility jumps, **only** the dynamic thresholds re-calibrate.
    """)

with tab2:
    st.header(f"Threshold Dashboard — {sel_cross}")
    c0,c1,c2,c3,c4,c5 = st.columns(6)
    c0.metric("Manual Thr",      f"{man_thr:.4f}")
    c1.metric("Latest Vol",      f"{dfc['Vol'][-1]:.4f}")
    c2.metric("Warn Thr",        f"{dfc['Thr_Warning'][-1]:.4f}")
    c3.metric("Alert Thr",       f"{dfc['Thr_Alert'][-1]:.4f}")
    c4.metric("Crit Thr",        f"{dfc['Thr_Critical'][-1]:.4f}")
    c5.metric("ML Consensus",    f"{feat['ML_Consensus'].mean():.1%}")

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=dfc.index,y=dfc['Vol'],name='Vol',line=dict(color='blue')))
    fig.add_trace(go.Scatter(x=dfc.index,y=dfc['Thr_Alert'],name='Alert',line=dict(color='orange',dash='dash')))
    fig.add_trace(go.Scatter(x=dfc.index,y=dfc['Thr_Critical'],name='Critical',line=dict(color='red',dash='dot')))
    fig.add_hline(y=man_thr,line_dash='longdash',line_color='black',
                  annotation_text="Manual",annotation_position="bottom right")
    fig.update_layout(xaxis=dict(rangeslider=dict(visible=True)),
                      yaxis_title="Daily Volatility",height=500)
    st.plotly_chart(fig,use_container_width=True)

with tab3:
    st.header("🧠 ML & Consensus Breach Rates")
    br = pd.Series({
        'Manual Thr':   (dfc['Vol']>man_thr).mean(),
        'Dyn Alert':    (dfc['Vol']>dfc['Thr_Alert']).mean(),
        'Dyn EVT':      (dfc['Vol']>dfc['Thr_Critical']).mean(),
        'ML Consensus': feat['ML_Consensus'].mean()
    })
    st.subheader("Overall Breach Comparison")
    st.bar_chart(br)

    st.subheader("ML Consensus over Time")
    fig2 = go.Figure()
    fig2.add_trace(go.Scatter(x=feat.index,y=feat['ML_Consensus'],
                              mode='lines',name='Consensus',line=dict(color='purple')))
    for nm,col in [('Manual','black'),('Dyn Alert','orange'),('Dyn EVT','red')]:
        fig2.add_hline(y=br[nm+" Thr" if nm=="Manual" else nm],
                       line_dash='dot',line_color=col,
                       annotation_text=nm,annotation_position="top left")
    fig2.update_layout(yaxis_title='Breach Rate / Consensus Flag',
                       xaxis_title="Date",height=400)
    st.plotly_chart(fig2,use_container_width=True)

with tab4:
    st.header("🚀 Shock Simulation & Re-calibration")

    # 1) date pickers on cross index range
    min_d = idx.min().date(); max_d = idx.max().date()
    snap_date  = st.date_input(
        "Snapshot Date (pre-shock)",
        value=(max_d - BDay(30)).date(),
        min_value=min_d, max_value=(max_d - datetime.timedelta(days=1))
    )
    shock_date = st.date_input(
        "Shock End Date",
        value=max_d,
        min_value=(snap_date + datetime.timedelta(days=1)), max_value=max_d
    )

    snap_ts  = max(d for d in idx if d.date()<=snap_date)
    shock_ts = max(d for d in idx if d.date()<=shock_date)

    # 2) shock settings
    shock_leg    = st.multiselect("Which leg(s) to shock?",['Base Leg','Quote Leg'],default=['Quote Leg'])
    shock_factor = st.slider("Shock Magnitude (× real vol)",1.0,5.0,2.0)

    # 3) apply synthetic shock only on cross idx
    b2 = b_ser.loc[idx].copy()
    q2 = q_ser.loc[idx].copy()
    mask = (b2.index>snap_ts)&(b2.index<=shock_ts)
    if 'Base Leg' in shock_leg:  b2.loc[mask] *= shock_factor
    if 'Quote Leg'in shock_leg:  q2.loc[mask] *= shock_factor
    vol2 = np.sqrt(b2**2 + q2**2)

    # 4) recalculate dynamic thresholds on shocked series
    dfd = dfc.copy(); dfd['Vol'] = vol2
    dfd['Thr_Alert_shock']=np.nan; dfd['Thr_Critical_shock']=np.nan
    for r,grp in dfd.groupby('Regime'):
        w,q = calib[r]['window'],calib[r]['quantile']
        dfd.loc[grp.index,'Thr_Alert_shock']    = rolling_quantile(grp['Vol'],w,q)
        dfd.loc[grp.index,'Thr_Critical_shock'] = evt_vol_threshold(grp['Vol'],0.90,calib[r]['tail'])

    # 5) snapshot vs shock values
    alert_snap = dfc.loc[snap_ts,'Thr_Alert']
    evt_snap   = dfc.loc[snap_ts,'Thr_Critical']
    alert_sh   = dfd.loc[shock_ts,'Thr_Alert_shock']
    evt_sh     = dfd.loc[shock_ts,'Thr_Critical_shock']

    c0,c1,c2,c3,c4 = st.columns(5)
    c0.metric("Manual Thr",       f"{man_thr:.4f}")
    c1.metric("Alert@Snap",       f"{alert_snap:.4f}")
    c2.metric("EVT@Snap",         f"{evt_snap:.4f}")
    c3.metric("Alert@Post-Shock", f"{alert_sh:.4f}")
    c4.metric("EVT@Post-Shock",   f"{evt_sh:.4f}")

    # 6) visualization
    fig3 = go.Figure()
    fig3.add_trace(go.Scatter(x=dfc.index,y=dfc['Vol'],name='Orig Vol',line=dict(color='lightgrey')))
    fig3.add_trace(go.Scatter(x=vol2.index,y=vol2,name='Shocked Vol',line=dict(color='blue')))
    fig3.add_vrect(x0=snap_ts,x1=shock_ts,fillcolor='red',opacity=0.1,line_width=0)
    fig3.add_hline(y=man_thr,line_dash='longdash',line_color='black',
                   annotation_text="Manual",annotation_position="bottom right")
    fig3.add_hline(y=alert_snap,line_dash='dash',line_color='orange',
                   annotation_text="Alert@Snap",annotation_position="bottom right")
    fig3.add_hline(y=alert_sh,line_dash='dash',line_color='orange',
                   annotation_text="Alert@Post",annotation_position="top right")
    fig3.add_hline(y=evt_snap,line_dash='dot',line_color='red',
                   annotation_text="EVT@Snap",annotation_position="bottom right")
    fig3.add_hline(y=evt_sh,line_dash='dot',line_color='red',
                   annotation_text="EVT@Post",annotation_position="top right")
    fig3.update_layout(xaxis_title="Date",yaxis_title="Daily Volatility",height=500)
    st.plotly_chart(fig3,use_container_width=True)

    st.markdown(f"""
    **Shock Window:** {snap_date:%Y-%m-%d} → {shock_date:%Y-%m-%d}  
    - Manual thresholds remain **static**.  
    - Dynamic thresholds **re-calibrate** with new volatility: Alert {alert_snap:.3f}→{alert_sh:.3f}, EVT {evt_snap:.4f}→{evt_sh:.4f}.  
    - Shaded area = synthetic volatility spike.
    """)
